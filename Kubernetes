r1) Difference between Docker vs kubernetes?
A) Docker is a container platform it makes container journer easy using docker engine or CLI. kubernetes is container orchestration platform

problem1: ( All are in single host)
containers are Ephemeral(shortlife in nature) die & revive anytime. if you have a host we have 100 containers in one the containers is taking lot of memory it may effects others and even other may not start may be lack of memory.
there is no relation ship between container 1 and 100 but still c1 kills c100 not directly but indirectly. linux has to decide based on algorithm to delete the which process it cant randomly delete 50.

problem2: (if someone killed the container) solved by Auto Healing
some devops enginner should start manually as application is not reachable. there 100 of reasons why a container goes down. we cant see all 10000 conatiners. containers are not healing automatically.

problem3: (AutoScaling)
Suppose host has 4cpu and 4GBmemory , max c1 container can have is 4gb and 4Gb memory. suppose during festival occasion there is 1million load we have to increase the load from 1 to 10 containers or it has to be scaled automatically but docker does not do it.
also we have to do load balancer for redirect first 10000 to c1 and next to others containers but docker cant do it.

Docker does not provide any enterprise level support. we need to have these enterprise level standards (1)Load balancer (2) Firewall (3)Autoscaling (4)Auto Heal (5)API Gateway should be supported for application
(we have to go to go to docker swarn earlier)

so the docker is never used in production as its not enterprise ready , but they may use docker swarn in production 

Kubernetes solves all above problems.
------------------------------------------------------------------------------------------------------------------------------------------------------------
1) Host: by default kubernetes is a cluster(group of nodes) its a master-node architecture. suppose there is problem with container1 and it using high memory kubernetes will shift the container to another node with out effecting from other
container1 because it has multiple nodes in the cluster.
2) Kubnertes has replicaset (earlier called as Replica controller). we dont have to deploy new container. kubernetes is all about YAML files. in Deployment.yml file we can say increase my traffic 1 to 10. 
also kubernetes has Horizontal Pod  Autoscaling(HPA). whenever my container has more than 70% traffic spin up the new container.

3) Kubernetes has to control the damage or Fix the Damage of Container Down using Autohealing even before this problem container goes does the "API Controller" container will rollout a new container the enduser may not known that container is down 
unless its a heavy application but general applications cant be identified.

4) Kubernetes is one of the part of borg used by google which is Enterprise level. (kubernetes can whilelist and blacklist ip that are doing Ddos attacks)
--------------------------------------------------------------------------------------------------------------------
CNCF(community for development):
by deafult kubernetes does not support advanced load balacing capabilities it has services and kube-proxy that has basic round-robbin, but by using custom resources and custom resource defnitions , kubebetes says to applications like  nginx you 
create kubernetes controller using which people can use your loadbalancer even inside kubernetes this concept is called ingress controllers.

-------------------------------------------------------------------------------------------------------------------------------
in Docker simplest thing is container: we have run a container using dcoker run , if there si java applicaion it will not run without java run time, similar in docker there is a "container runtime" called dockershim this is 
happening under the hood in docker.


in k8's the simplest thing is pod:
in kubernetes we create a master and workers(there will be multiple master and workers in production). in k8's we dont send request directly we send through master our request goes to Controlplane. pod is like wrapper around the container
which has some additional capabilities. 


Architecture of Kubernetes:

Dataplane:
when deploy a pod , pod gets deployed in a workernode in which kubelet is resposbile for maintaining the pod if anything goes wrong it will notify the API_Server in master that pod is down.
because k8 has auto-healing capabilities kublet has to inform that pod is not running do something to API-Server.
in  kubernetes inside the pod there is container and to run the container we can use Dockershim of docker or advanced like Container-d or CRI-O as a container runtime or any other container runtimes which implemets k8 runtime interface.

in docker there is only dockershim but in k8s we have multiple options
in docker there is Docker0 in k8 there is kubeproxy which provide every pod that is allocated with  ip-address,networking and default loadbalancing capabilities. as k8 has auto-scaling capabilitie kube-proxy send 50% trafiic
to one and another 50% to another. generally kube-proxy uses ip-tables on linux machines for networking realated configuration.

controlplane: the reason for existing of controlplane is to main standards ,cluster is one standard of enterprise who will decide in which node pod has to be created this is just one instruction like this
there will be multiple instructions in k8 the core component that deals with all these when multiple people are trying to accessing k8 cluster called API Server. so the ApI is a component that basically exposes
our k8 to the outer-world which takes requets from external world. (but all services in dataplane are not expose to outerworld users does not know there fuctinality).

lets say user is trying to create a pod. he tries to access the api-server teh api-server knows that node1 is free, but to schedule the component on node1 you have component called kube scheduler.
(api server receives the information and kube scheduler will do the action as soon it received the request from api-server)

etcd: let say we are deploying production level services in k8 which it acts keyvalue store which backup and stores entire cluster information,  data is stored as object or key-value pair. what happens
without etcd we dont have cluster related information. suppose we want to restore the information etcd is the basic component.

controller manager: basically k8 supports autoscaling, to support autoscaling k8 has some components like to automatically detect it for that k8 has some controllers. for example replica-set which maintains state of 
k8 pods. suppose if one pod is not enough i have autoscaled to 2, the component that makesure both the pods run is replica-sets. so control-manager make sure all components like replica sets will be running.
in k8 yaml file if i say i need 2 pods replicaset controller make sure 2 pods will run.

cloud-contoller-manager(ccm): (eks,aks,gke) suppose in eks user want storage this ccm identifies which cloud provider it is and deploys it. suppose a new competeter comes to cloud he has to use k8 he has to define 
his own logics in ccm open-source github. if we use in on-premise we dont need this ccm component to be created at all in cluster.












Pod - A Kubernetes pod is a collection of one or more containers, and is the smallest unit of a Kubernetes application

# Control plane (master node)
## api server - core component of k8s, accepts all incoming  reqs, exposes k8s to external world.
## etcd - key value store, cluster related infos.
## scheduler - scheduling pods or resources on k8s, receives info from api server & acts on it
## controller manager - ensures controllers like replica set are running
## cloud controller manager - like terraform

# Data plane (worker node)
## kubelet - creates pod, ensures pod is always running
## kube proxy - provides networking like Docker0, default load balancing
## container runtime - runs container inside pod.
-------------------------------------------------------------------------------------------------------------------
k8's production systems:  life cycle of k8 cluster are create,upgrade,configuration and delete
(people do on local like minikube,k38,kind,micro8ks,k3d they dont have HA , also they don't have production issues.)

popular kubernetes distribution: for any opensource there wiil be distribution for linux opensource(redhat,amazon linux are some distributions of linux who provides regular 
updates and security patches is there are any issue we can go to oracle or linux for support).
like that for k8's open source some distributions are EKS,Openshift,Tanzu,Rancher.

order of distributions by popularity:
1) Kubernetes
2) Openshift
3) Rancher
4) VMware - Tanzu
5) Eks, Aks, Gke,DKE


in realtime people uses k8's for staging and pre-pod and production they uses Eks etc to reduce costs. some may use k8 in production also if they businnes is not time dependent
(even if k8,s is down for sometime there is no problem).

kops(k8's operations):  for installing k8's in real-world, the only difference between k8's and eks suppose if we install k8's in aws if we are in to anything they dont provide support 
but eks they provide support and also they have additional wrappers like eksctl, some additional plugins and command line options and scripts but functionality is same.

-----------------------------------------------------
how we manage 100's of cluster in realtime.
Kops is most widely used tools, we also have (Kubeadm(old, we have to do lot of manual activity, upgrades,modifying clusters is not that smooth), kubesquash).

------------------------------------------------------------------------------------
python3, aws cli are dependencies.
kubectl: to play with k8s cluster.
----------------------------------------------------------------
created aws bucket from cli : aws s3api create-bucket --bucket kops-raghu-storage --region us-east-1
-----------------------------------------------------------------------------------------------------------------
kubernetes says dont install your container directly but run as pod. in kubernates we pass the arguments in docker command in yaml file of pod that the only difference.
pod is 1 (or) group of container (mostly we keep multiple container for side-car, init)
like in side-car we use api-gateway rules and loadbalancer rules)  populr use case is servive-mesh which we use side car container
if we keep 2 containers in same pod they can share network and shared storage and shared files but this is rare case.
ipaddresses are assignned for pods not for container.
we are writing same docker commands of CLI in YAML for Declarative syntax.








init and sidecar:

Timing: Init containers run and complete before the main application container starts, whereas sidecar containers run alongside the main container.
Purpose: Init containers are used for setup and initialization tasks, while sidecar containers extend or enhance the main container's functionality during its runtime.
Dependencies: Init containers must complete successfully before the main container starts, ensuring that any dependencies or setup tasks are fulfilled before the main application begins execution.


what is kubectl?
kubectl is command line tool for k8's to interact with k8 clusters.

to see number of nodes: kubectl get nodes
to see number of pods: kubectl get pods
--------------------------------------------------------------------------------------------------------------------
not for realtime:
minikube creates vm and on top in it creates single node kubenetes cluster.
minikube start --memory=4096  --driver=hyperkit
Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

to login in learning cluster:minikube ssh
to check pod is live or not: curl 10.244.0.3
----------------------------------------------------------
below is pod.yaml file:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80


----------------------------------------------------------------------------------------
to create pod: kubectl create -f pod.yml
to see number of pods: kubectl get pods
to delete pod: kubectl delete pod nginx


to see more detiails of pod: kubectl get pods -o wide
inreatime we will ssh to k8 master or node ip.
---------------------------------------------------------------------------------------------------------------------
how to get auto scaling and auto healing in kubernetes:
on top of pod we have a wrapper called deployment to get features of auto scaling and auto healing.

we will say Kind: deployment in yaml file instead of pod

in realtime this is way to deploy our apps new never do kind:pods, we will do deployments,satefulsets,daemon sets

to see logs of pod: kubectl logs nginx
to see all info of pod(even to debug we use same command): kubectl describe pod nginx
------------------------------------------------------------
how to debug pods?
A) we use "kubectl describe pod nginx" and "kubectl logs nginx"    here nginx is name of pod.

--------------------------------------------------------------------------------------------------------------------------
difference between container , POD and deployment?
container to run we use cli for running
pod use manifest(pod.yaml) for enterprise level support,  a pod can be single or multiple container. if you use a  pod we can share the resouces,volumes and same network.
deployment we use for auto-healing and auto-scaling ( if we create a deployment it will create intermediate deployment called replica sets(this is kubernetes controller) this controller will create pod at last)
(we can just say inside depl0yment we can mention no of replicas to multiple concurrent users.(we call it HA and load balancing), if a user says i have deleted pod accidently k8's by using controller(replica set)
will still recreate the second deleted pod by user automatically by auto healing by using deployment manifest file we have submitted.

controller make sures desired state is always maintained in the actual cluster from manifiest yaml file we have given to k8. any thing the maintains the state is a controller 
there are many controllers in k8's some are default and some custom like argo-cd, admission controllers.


so dont create the pod directly.
----------------------------------------------------------
difference between deployment and replicaset.
A) whenever we create a deployment , a replica set is automatically created which is a controller which manages the auto-healing on the state on basis manifest yaml  file that is submitted during deployment.

----------------------------------------------------------------------------------------------------

to get all the details of resources of a particular namespace in realtime we use: kubectl get all
to get all the details for the name spaces use: kubectl get all -a

--------------------------------
below is code for delpoyment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
---------------------------------------------------------------------------------------
to create deplyment: kubectl apply -f deployment.yml   (-f flag stands for file that contains the configuration to be used)
to get deploy: kubectl get deploy
to get replica set: kubectl get rs
to get pod: kubectl get pods

kubernetes controler is nothing but a golanguage application which was written by k8 team to ensure the desired state is avaiable in cluster.
-----------------------------------------------------------------------------------------
to watch pods it will show you whats happening with pods: kubectl get pods -w


-----------------------------------------------------------------------------------------------------------------------------------------
k8s Service: ((1)Load Balancing (2)Service discovery (3)Expose to world)
suppose we created 3 replicas for 100 concurrent users i.e at the same time ,to reduce the load of all the traffic on single pod  we distribute the traffic among 3 we use replica so that load is shared.
if any one ask what is count of your pods you can it depends on no of users if one pod accepts up to 10 users then if 100 users comes we need 10 pods. developers and devops will take this decision.
in the world of k8s or containers a pod going down is very common. but k8s had auto healing as soon as pod is getting deleted new pod comes before the deletion of earlier pod. but the ip-address of new pod 
is different than earlier. suppose we gave project1 to test to a team we gave ip 172.16.3.4 but the pod was deleted and new pod came the ip was 172.16.3.5 but test team was accesing earlier ip they will have error.
so to avoid above issue on top of deployment------------we create service(svc:) what service will do is it acts as a Loadbalancer by using a component called kubeproxy. so instead accessing applications
by ip we access it by service name like "payments.default.svc". so the service using kubeproxy will forward the requests to ipaddress by sharing.

so without services applications are not accesable for some users.
--------------------
we should get question how service will identify the new pod which ipaddress han changed , how it will direct the users to new pod with new ipaddress. to overcome this instead of using
ipaddress which changes frequently service uses service discovry by using labels and selectors. what is this labels and selectors is for every pod that is getting created devlopers will apply
a label for all the same pods, like a label payment for all payment pods. so the service will keep track of label instead of ipaddress when replicaset creates a pod then this problrm is solved as new pod receives
the name as it manifestfile. (we will just mention label in metadata for service discovery)
------------------------
Service can expose our application outside the k8's cluster, (note: deployment can give access to app byr login to ssh to master or node it cant expose to outer world)

whenever we are creating a k8 service resouce in yaml manifest we can create service from 3 default types below.
1) cluster ip (with in k8 cluster only we access the app, this is default behaviour we get 2 benefits discovery and load balancing, outside users cant access, devops enginner etc who has access)
2) Node port (with in the organization we can access the app, it will allow anyone who has access to any master, worker node 1 or 2 or 3 or people who has access to org vpn and node and k8 networking can access)
3) Load balancer (entire world)
suppose we are deploying service type as Load balancer in EKS, we get elastic load balancer ipaddress. they can access using public ip address
4)ExternalName
5) headless service 




------------------------------------------------------------------------------------------------------------------------------------------------------------
K8s networking:
choose Flannel for simplicity and basic networking needs, and opt for Calico when you need advanced network policies and potentially better performance.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1) difference between docker and k8's
2) architecture of k8's
3) difference bettween docker swarm and kubernetes?
A) docker swarm is dcoker based solutions for simple scale application or small scale companies. k8's can do advance networking capabilities and also we have lot of 3 rd party commmunity ver high for k8's.
also for large scale enterprise companies k8's is best.
4) difference between k8's pd and docker container?
5) what is an namespace in k8's?
A) In Kubernetes namespace is a logical isolation of resources, network policies, rbac and everything. For example, there are two projects using same k8s cluster. 
One project can use ns1 and other project can use ns2 without any overlap and authentication problems.
namespace is nothing but logical isolation of network, resources and Rbac etc. we can restrict resources sharing other namespace resources.
6) what is the roles of kube-proxy?
A) Kube-proxy works by maintaining a set of network rules on each node in the cluster, which are updated dynamically as services are added or removed. When a client sends a request to a service, the request is intercepted by kube-proxy on the node where it was received. Kube-proxy then looks up the destination endpoint for the service and routes the request accordingly.
Kube-proxy is an essential component of a Kubernetes cluster, as it ensures that services can communicate with each other.

7) what are different types of services with in k8s?
A) 1)Clustet IP Mode, 2)Node port Mode and 3)Load balancer Mode

8) What is the difference between NodePort and LoadBalancer type service ?
When a service is created a NodePort type, The kube-proxy updates the IPTables with Node IP address and port that is chosen in the service configuration to access the pods.
Where as if you create a Service as type LoadBalancer, the cloud control manager creates a external load balancer IP using the underlying cloud provider logic in the C-CM.
Users can access services using the external IP

9) what is the roles of kubelet?
A) Kubelet manages the containers that are scheduled to run on that node. It ensures that the containers are running and healthy, and that the resources they need are available.
Kubelet communicates with the Kubernetes API server to get information about the containers that should be running on the node, and then starts and stops the containers as needed to maintain the desired state. 
It also monitors the containers to ensure that they are running correctly, and restarts them if necessary.

10) day to day actvities on k8s?
A) As part of my role, we manage k8s clusters and applications that are deployed in k8s are running correctly and we also monitor them and esure when they are bugs like developers are not able to trubleshoot services. whe also developers team is unable to route the traffice in k8s
we resolve them in k8s cluter. we also do maintaining activities we have kubernets cluster with  3 master nodes and 10 worker nodes we have to do contious activites on worker nodes updraging versions of worker nodes.
installing mandatory packages and make sure workers nodes are not exposed to nay security vunarabilities. if anyone has any issue with k8's they create jira items / tickets we try to resolve the.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

to get all deployments: kubectl get deployments

to delete deplyment: kubectl delete deployment nginx-deployment

build docker images: docker build -t vdeepikavr/python-sample-app-demo:v1 
----------------
push to repository:
docker tag vdeepikavr/python-sample-app-demo:v1 vdeepikavr/python-sample-app-demo:v1
docker push your-dockerhub-username/python-sample-app-demo:v1
--------------------------------------------------------------------------------------------------------------------------------------------
kubectl apply -f pyappdeply.yml

kubectl get deploy

kubectl get pods -o wide

to get full info about api call : kubectl get pods -v=7 or kubectl get pods -v=9

-----------------------------------------------------------------------------------------------------------------------------------------------------

minikube ssh
curl -L http://10.244.0.19:8000/demo
----------------------------------------------------------------------------------
mention version of app in deplyment.yml
image: vdeepikavr/python-sample-app-demo:v1   # Added tag v1

-----------------------------------
service.yml:
apiVersion: v1
kind: Service
metadata:
  name: python-raghu-servive-one
spec:
  type: NodePort
  selector:
    app: sample-python-label-app
  ports:
    - port: 80
      # By default and for convenience, the `targetPort` is set to
      # the same value as the `port` field.
      targetPort: 8000
      # Optional field
      # By default and for convenience, the Kubernetes control plane
      # will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
-------------------------
deplyment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-python-appnname
  labels:
    app: sample-python-label-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sample-python-label-app
  template:
    metadata:
      labels:
        app: sample-python-label-app
    spec:
      containers:
      - name: python-appname
        image: vdeepikavr/python-sample-app-demo:v1   # Added tag v1
        ports:
        - containerPort: 8000
----------------------------------------------------------
in service.yml change the target port to 8000 (or port which our application is running

service works on labels not ip


kubectl apply -f service.yml
kubectl get svc
---
minikube ssh
curl -L http://10.107.204.76:80/demo
---

to get the ip of minikube node: minikube ip
192.168.49.2
curl -L http://192.168.64.10:30007/demo
now app can be access from above url also
------------------------------------------------------------
to change to load balancer:

kubectl get svc
kucectl edit svc python-raghu_servive-one
---you will see the "type: NodePort" change it to "type: LoadBalancer"

it will work only in cloud not in minikube
-------------------------------------------------------------------
if you want to see the packet flow install "kubeshark" in k8's, to see how rquest flows.
know about wireshark and tcp dump
------------------------------------------------------------------------------


INGRESS(38):

before 2015 version 1.1 , ingress was not there

people used to do load balancing using nginx or F5 these are Enterprise LoadBalncers they option like 
RatioBased load balancing: send 3 requets to node1 and 1 to node2 etc
Sticy Session: if a user first request goes to node1 the next request also should go to node1 only
pathbased load balacing: Route /api requests to API Server and /app requests to Web Server.
domain: Route example.com to App Server 1 and api.example.com to App Server 2.
whitelisting : only allow specific customers to allow apps
black list: donot allow users from uk
--------------------------------------------

service was providing load balancing mechanism but thatw as simple round robbin machanism (suppose if there are 2 nodes and 10 requests 5 req goto node1 and other 5 to node 2 this simple load balancing).






















































