r1) Difference between Docker vs kubernetes?
A) Docker is a container platform it makes container journer easy using docker engine or CLI. kubernetes is container orchestration platform

problem1: ( All are in single host)
containers are Ephemeral(shortlife in nature) die & revive anytime. if you have a host we have 100 containers in one the containers is taking lot of memory it may effects others and even other may not start may be lack of memory.
there is no relation ship between container 1 and 100 but still c1 kills c100 not directly but indirectly. linux has to decide based on algorithm to delete the which process it cant randomly delete 50.

problem2: (if someone killed the container) solved by Auto Healing
some devops enginner should start manually as application is not reachable. there 100 of reasons why a container goes down. we cant see all 10000 conatiners. containers are not healing automatically.

problem3: (AutoScaling)
Suppose host has 4cpu and 4GBmemory , max c1 container can have is 4gb and 4Gb memory. suppose during festival occasion there is 1million load we have to increase the load from 1 to 10 containers or it has to be scaled automatically but docker does not do it.
also we have to do load balancer for redirect first 10000 to c1 and next to others containers but docker cant do it.

Docker does not provide any enterprise level support. we need to have these enterprise level standards (1)Load balancer (2) Firewall (3)Autoscaling (4)Auto Heal (5)API Gateway should be supported for application
(we have to go to go to docker swarn earlier)

so the docker is never used in production as its not enterprise ready , but they may use docker swarn in production 

Kubernetes solves all above problems.
------------------------------------------------------------------------------------------------------------------------------------------------------------
1) Host: by default kubernetes is a cluster(group of nodes) its a master-node architecture. suppose there is problem with container1 and it using high memory kubernetes will shift the container to another node with out effecting from other
container1 because it has multiple nodes in the cluster.
2) Kubnertes has replicaset (earlier called as Replica controller). we dont have to deploy new container. kubernetes is all about YAML files. in Deployment.yml file we can say increase my traffic 1 to 10. 
also kubernetes has Horizontal Pod  Autoscaling(HPA). whenever my container has more than 70% traffic spin up the new container.

3) Kubernetes has to control the damage or Fix the Damage of Container Down using Autohealing even before this problem container goes does the "API Controller" container will rollout a new container the enduser may not known that container is down 
unless its a heavy application but general applications cant be identified.

4) Kubernetes is one of the part of borg used by google which is Enterprise level. (kubernetes can whilelist and blacklist ip that are doing Ddos attacks)
--------------------------------------------------------------------------------------------------------------------
CNCF(community for development):
by deafult kubernetes does not support advanced load balacing capabilities it has services and kube-proxy that has basic round-robbin, but by using custom resources and custom resource defnitions , kubebetes says to applications like  nginx you 
create kubernetes controller using which people can use your loadbalancer even inside kubernetes this concept is called ingress controllers.

-------------------------------------------------------------------------------------------------------------------------------
in Docker simplest thing is container: we have run a container using dcoker run , if there si java applicaion it will not run without java run time, similar in docker there is a "container runtime" called dockershim this is 
happening under the hood in docker.


in k8's the simplest thing is pod:
in kubernetes we create a master and workers(there will be multiple master and workers in production). in k8's we dont send request directly we send through master our request goes to Controlplane. pod is like wrapper around the container
which has some additional capabilities. 


Architecture of Kubernetes:

Dataplane:
when deploy a pod , pod gets deployed in a workernode in which kubelet is resposbile for maintaining the pod if anything goes wrong it will notify the API_Server in master that pod is down.
because k8 has auto-healing capabilities kublet has to inform that pod is not running do something to API-Server.
in  kubernetes inside the pod there is container and to run the container we can use Dockershim of docker or advanced like Container-d or CRI-O as a container runtime or any other container runtimes which implemets k8 runtime interface.

in docker there is only dockershim but in k8s we have multiple options
in docker there is Docker0 in k8 there is kubeproxy which provide every pod that is allocated with  ip-address,networking and default loadbalancing capabilities. as k8 has auto-scaling capabilitie kube-proxy send 50% trafiic
to one and another 50% to another. generally kube-proxy uses ip-tables on linux machines for networking realated configuration.

controlplane: the reason for existing of controlplane is to main standards ,cluster is one standard of enterprise who will decide in which node pod has to be created this is just one instruction like this
there will be multiple instructions in k8 the core component that deals with all these when multiple people are trying to accessing k8 cluster called API Server. so the ApI is a component that basically exposes
our k8 to the outer-world which takes requets from external world. (but all services in dataplane are not expose to outerworld users does not know there fuctinality).

lets say user is trying to create a pod. he tries to access the api-server teh api-server knows that node1 is free, but to schedule the component on node1 you have component called kube scheduler.
(api server receives the information and kube scheduler will do the action as soon it received the request from api-server)

etcd: let say we are deploying production level services in k8 which it acts keyvalue store which backup and stores entire cluster information,  data is stored as object or key-value pair. what happens
without etcd we dont have cluster related information. suppose we want to restore the information etcd is the basic component.

controller manager: basically k8 supports autoscaling, to support autoscaling k8 has some components like to automatically detect it for that k8 has some controllers. for example replica-set which maintains state of 
k8 pods. suppose if one pod is not enough i have autoscaled to 2, the component that makesure both the pods run is replica-sets. so control-manager make sure all components like replica sets will be running.
in k8 yaml file if i say i need 2 pods replicaset controller make sure 2 pods will run.

cloud-contoller-manager(ccm): (eks,aks,gke) suppose in eks user want storage this ccm identifies which cloud provider it is and deploys it. suppose a new competeter comes to cloud he has to use k8 he has to define 
his own logics in ccm open-source github. if we use in on-premise we dont need this ccm component to be created at all in cluster.












Pod - A Kubernetes pod is a collection of one or more containers, and is the smallest unit of a Kubernetes application

# Control plane (master node)
## api server - core component of k8s, accepts all incoming  reqs, exposes k8s to external world.
## etcd - key value store, cluster related infos.
## scheduler - scheduling pods or resources on k8s, receives info from api server & acts on it
## controller manager - ensures controllers like replica set are running
## cloud controller manager - like terraform

# Data plane (worker node)
## kubelet - creates pod, ensures pod is always running
## kube proxy - provides networking like Docker0, default load balancing
## container runtime - runs container inside pod.
-------------------------------------------------------------------------------------------------------------------
k8's production systems:  life cycle of k8 cluster are create,upgrade,configuration and delete
(people do on local like minikube,k38,kind,micro8ks,k3d they dont have HA , also they don't have production issues.)

popular kubernetes distribution: for any opensource there wiil be distribution for linux opensource(redhat,amazon linux are some distributions of linux who provides regular 
updates and security patches is there are any issue we can go to oracle or linux for support).
like that for k8's open source some distributions are EKS,Openshift,Tanzu,Rancher.

order of distributions by popularity:
1) Kubernetes
2) Openshift
3) Rancher
4) VMware - Tanzu
5) Eks, Aks, Gke,DKE


in realtime people uses k8's for staging and pre-pod and production they uses Eks etc to reduce costs. some may use k8 in production also if they businnes is not time dependent
(even if k8,s is down for sometime there is no problem).

kops(k8's operations):  for installing k8's in real-world, the only difference between k8's and eks suppose if we install k8's in aws if we are in to anything they dont provide support 
but eks they provide support and also they have additional wrappers like eksctl, some additional plugins and command line options and scripts but functionality is same.

-----------------------------------------------------
how we manage 100's of cluster in realtime.
Kops is most widely used tools, we also have (Kubeadm(old, we have to do lot of manual activity, upgrades,modifying clusters is not that smooth), kubesquash).

------------------------------------------------------------------------------------
python3, aws cli are dependencies.
kubectl: to play with k8s cluster.
----------------------------------------------------------------
created aws bucket from cli : aws s3api create-bucket --bucket kops-raghu-storage --region us-east-1
-----------------------------------------------------------------------------------------------------------------
kubernetes says dont install your container directly but run as pod. in kubernates we pass the arguments in docker command in yaml file of pod that the only difference.
pod is 1 (or) group of container (mostly we keep multiple container for side-car, init)
like in side-car we use api-gateway rules and loadbalancer rules)  populr use case is servive-mesh which we use side car container
if we keep 2 containers in same pod they can share network and shared storage and shared files but this is rare case.
ipaddresses are assignned for pods not for container.
we are writing same docker commands of CLI in YAML for Declarative syntax.








init and sidecar:

Timing: Init containers run and complete before the main application container starts, whereas sidecar containers run alongside the main container.
Purpose: Init containers are used for setup and initialization tasks, while sidecar containers extend or enhance the main container's functionality during its runtime.
Dependencies: Init containers must complete successfully before the main container starts, ensuring that any dependencies or setup tasks are fulfilled before the main application begins execution.


what is kubectl?
kubectl is command line tool for k8's to interact with k8 clusters.

to see number of nodes: kubectl get nodes
to see number of pods: kubectl get pods
--------------------------------------------------------------------------------------------------------------------
not for realtime:
minikube creates vm and on top in it creates single node kubenetes cluster.
minikube start --memory=4096  --driver=hyperkit
Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

to login in learning cluster:minikube ssh
to check pod is live or not: curl 10.244.0.3
----------------------------------------------------------
below is pod.yaml file:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.14.2
    ports:
    - containerPort: 80


----------------------------------------------------------------------------------------
to create pod: kubectl create -f pod.yml
to see number of pods: kubectl get pods
to delete pod: kubectl delete pod nginx


to see more detiails of pod: kubectl get pods -o wide
inreatime we will ssh to k8 master or node ip.
---------------------------------------------------------------------------------------------------------------------
how to get auto scaling and auto healing in kubernetes:
on top of pod we have a wrapper called deployment to get features of auto scaling and auto healing.

we will say Kind: deployment in yaml file instead of pod

in realtime this is way to deploy our apps new never do kind:pods, we will do deployments,satefulsets,daemon sets

to see logs of pod: kubectl logs nginx
to see all info of pod(even to debug we use same command): kubectl describe pod nginx
------------------------------------------------------------
how to debug pods?
A) we use "kubectl describe pod nginx" and "kubectl logs nginx"    here nginx is name of pod.

--------------------------------------------------------------------------------------------------------------------------
difference between container , POD and deployment?
container to run we use cli for running
pod use manifest(pod.yaml) for enterprise level support,  a pod can be single or multiple container. if you use a  pod we can share the resouces,volumes and same network.
deployment we use for auto-healing and auto-scaling ( if we create a deployment it will create intermediate deployment called replica sets(this is kubernetes controller) this controller will create pod at last)
(we can just say inside depl0yment we can mention no of replicas to multiple concurrent users.(we call it HA and load balancing), if a user says i have deleted pod accidently k8's by using controller(replica set)
will still recreate the second deleted pod by user automatically by auto healing by using deployment manifest file we have submitted.

controller make sures desired state is always maintained in the actual cluster from manifiest yaml file we have given to k8. any thing the maintains the state is a controller 
there are many controllers in k8's some are default and some custom like argo-cd, admission controllers.


so dont create the pod directly.
----------------------------------------------------------
difference between deployment and replicaset.
A) whenever we create a deployment , a replica set is automatically created which is a controller which manages the auto-healing on the state on basis manifest yaml  file that is submitted during deployment.

----------------------------------------------------------------------------------------------------

to get all the details of resources of a particular namespace in realtime we use: kubectl get all
to get all the details for the name spaces use: kubectl get all -a

--------------------------------
below is code for delpoyment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
---------------------------------------------------------------------------------------
to create deplyment: kubectl apply -f deployment.yml   (-f flag stands for file that contains the configuration to be used)
to get deploy: kubectl get deploy
to get replica set: kubectl get rs
to get pod: kubectl get pods

kubernetes controler is nothing but a golanguage application which was written by k8 team to ensure the desired state is avaiable in cluster.
-----------------------------------------------------------------------------------------
to watch pods it will show you whats happening with pods: kubectl get pods -w


-----------------------------------------------------------------------------------------------------------------------------------------
k8s Service: ((1)Load Balancing (2)Service discovery (3)Expose to world)
suppose we created 3 replicas for 100 concurrent users i.e at the same time ,to reduce the load of all the traffic on single pod  we distribute the traffic among 3 we use replica so that load is shared.
if any one ask what is count of your pods you can it depends on no of users if one pod accepts up to 10 users then if 100 users comes we need 10 pods. developers and devops will take this decision.
in the world of k8s or containers a pod going down is very common. but k8s had auto healing as soon as pod is getting deleted new pod comes before the deletion of earlier pod. but the ip-address of new pod 
is different than earlier. suppose we gave project1 to test to a team we gave ip 172.16.3.4 but the pod was deleted and new pod came the ip was 172.16.3.5 but test team was accesing earlier ip they will have error.
so to avoid above issue on top of deployment------------we create service(svc:) what service will do is it acts as a Loadbalancer by using a component called kubeproxy. so instead accessing applications
by ip we access it by service name like "payments.default.svc". so the service using kubeproxy will forward the requests to ipaddress by sharing.

so without services applications are not accesable for some users.
--------------------
we should get question how service will identify the new pod which ipaddress han changed , how it will direct the users to new pod with new ipaddress. to overcome this instead of using
ipaddress which changes frequently service uses service discovry by using labels and selectors. what is this labels and selectors is for every pod that is getting created devlopers will apply
a label for all the same pods, like a label payment for all payment pods. so the service will keep track of label instead of ipaddress when replicaset creates a pod then this problrm is solved as new pod receives
the name as it manifestfile. (we will just mention label in metadata for service discovery)
------------------------
Service can expose our application outside the k8's cluster, (note: deployment can give access to app byr login to ssh to master or node it cant expose to outer world)

whenever we are creating a k8 service resouce in yaml manifest we can create service from 3 default types below.
1) cluster ip (with in k8 cluster only we access the app, this is default behaviour we get 2 benefits discovery and load balancing, outside users cant access, devops enginner etc who has access)
2) Node port (with in the organization we can access the app, it will allow anyone who has access to any master, worker node 1 or 2 or 3 or people who has access to org vpn and node and k8 networking can access)
3) Load balancer (entire world)
suppose we are deploying service type as Load balancer in EKS, we get elastic load balancer ipaddress. they can access using public ip address
4)ExternalName
5) headless service 




------------------------------------------------------------------------------------------------------------------------------------------------------------
K8s networking:
choose Flannel for simplicity and basic networking needs, and opt for Calico when you need advanced network policies and potentially better performance.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1) difference between docker and k8's
2) architecture of k8's
3) difference bettween docker swarm and kubernetes?
A) docker swarm is dcoker based solutions for simple scale application or small scale companies. k8's can do advance networking capabilities and also we have lot of 3 rd party commmunity ver high for k8's.
also for large scale enterprise companies k8's is best.
4) difference between k8's pd and docker container?
5) what is an namespace in k8's?
A) In Kubernetes namespace is a logical isolation of resources, network policies, rbac and everything. For example, there are two projects using same k8s cluster. 
One project can use ns1 and other project can use ns2 without any overlap and authentication problems.
namespace is nothing but logical isolation of network, resources and Rbac etc. we can restrict resources sharing other namespace resources.
6) what is the roles of kube-proxy?
A) Kube-proxy works by maintaining a set of network rules on each node in the cluster, which are updated dynamically as services are added or removed. When a client sends a request to a service, the request is intercepted by kube-proxy on the node where it was received. Kube-proxy then looks up the destination endpoint for the service and routes the request accordingly.
Kube-proxy is an essential component of a Kubernetes cluster, as it ensures that services can communicate with each other.

7) what are different types of services with in k8s?
A) 1)Clustet IP Mode, 2)Node port Mode and 3)Load balancer Mode

8) What is the difference between NodePort and LoadBalancer type service ?
When a service is created a NodePort type, The kube-proxy updates the IPTables with Node IP address and port that is chosen in the service configuration to access the pods.
Where as if you create a Service as type LoadBalancer, the cloud control manager creates a external load balancer IP using the underlying cloud provider logic in the C-CM.
Users can access services using the external IP

9) what is the roles of kubelet?
A) Kubelet manages the containers that are scheduled to run on that node. It ensures that the containers are running and healthy, and that the resources they need are available.
Kubelet communicates with the Kubernetes API server to get information about the containers that should be running on the node, and then starts and stops the containers as needed to maintain the desired state. 
It also monitors the containers to ensure that they are running correctly, and restarts them if necessary.

10) day to day actvities on k8s?
A) As part of my role, we manage k8s clusters and applications that are deployed in k8s are running correctly and we also monitor them and esure when they are bugs like developers are not able to trubleshoot services. whe also developers team is unable to route the traffice in k8s
we resolve them in k8s cluter. we also do maintaining activities we have kubernets cluster with  3 master nodes and 10 worker nodes we have to do contious activites on worker nodes updraging versions of worker nodes.
installing mandatory packages and make sure workers nodes are not exposed to nay security vunarabilities. if anyone has any issue with k8's they create jira items / tickets we try to resolve the.

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

to get all deployments: kubectl get deployments

to delete deplyment: kubectl delete deployment nginx-deployment

build docker images: docker build -t vdeepikavr/python-sample-app-demo:v1 
----------------
push to repository:
docker tag vdeepikavr/python-sample-app-demo:v1 vdeepikavr/python-sample-app-demo:v1
docker push your-dockerhub-username/python-sample-app-demo:v1
--------------------------------------------------------------------------------------------------------------------------------------------
kubectl apply -f pyappdeply.yml

kubectl get deploy

kubectl get pods -o wide

to get full info about api call : kubectl get pods -v=7 or kubectl get pods -v=9

-----------------------------------------------------------------------------------------------------------------------------------------------------

minikube ssh
curl -L http://10.244.0.19:8000/demo
----------------------------------------------------------------------------------
mention version of app in deplyment.yml
image: vdeepikavr/python-sample-app-demo:v1   # Added tag v1

-----------------------------------
service.yml:
apiVersion: v1
kind: Service
metadata:
  name: python-raghu-servive-one
spec:
  type: NodePort
  selector:
    app: sample-python-label-app
  ports:
    - port: 80
      # By default and for convenience, the `targetPort` is set to
      # the same value as the `port` field.
      targetPort: 8000
      # Optional field
      # By default and for convenience, the Kubernetes control plane
      # will allocate a port from a range (default: 30000-32767)
      nodePort: 30007
-------------------------
deplyment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-python-appnname
  labels:
    app: sample-python-label-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: sample-python-label-app
  template:
    metadata:
      labels:
        app: sample-python-label-app
    spec:
      containers:
      - name: python-appname
        image: vdeepikavr/python-sample-app-demo:v1   # Added tag v1
        ports:
        - containerPort: 8000
----------------------------------------------------------
in service.yml change the target port to 8000 (or port which our application is running

service works on labels not ip


kubectl apply -f service.yml
kubectl get svc
---
minikube ssh
curl -L http://10.107.204.76:80/demo
---

to get the ip of minikube node: minikube ip
192.168.49.2
curl -L http://192.168.64.10:30007/demo
now app can be access from above url also
------------------------------------------------------------
to change to load balancer:

kubectl get svc
kucectl edit svc python-raghu_servive-one
---you will see the "type: NodePort" change it to "type: LoadBalancer"

it will work only in cloud not in minikube
-------------------------------------------------------------------
if you want to see the packet flow install "kubeshark" in k8's, to see how rquest flows.
know about wireshark and tcp dump
------------------------------------------------------------------------------


INGRESS(38):

before 2015 version 1.1 , ingress was not there

people used to do load balancing using nginx or F5 these are Enterprise LoadBalncers they option like 
RatioBased load balancing: send 3 requets to node1 and 1 to node2 etc
Sticy Session: if a user first request goes to node1 the next request also should go to node1 only
pathbased load balacing: Route /api requests to API Server and /app requests to Web Server.
domain: Route example.com to App Server 1 and api.example.com to App Server 2.
whitelisting : only allow specific customers to allow apps
black list: donot allow users from uk
--------------------------------------------

service was providing load balancing mechanism but thatw as simple round robbin machanism (suppose if there are 2 nodes and 10 requests 5 req goto node1 and other 5 to node 2 this simple load balancing).


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
problem 1 in services: in realtime for amazon there will be 1000s of services for each service they have to assign staticipaddress  which cloud provider charges amount for each.
problem2:  also we are exposing all 1000 static ip outside.
--------------------------------------------------------
to sum up enterprise loadbalacing and tls in vms are missing in k8s

openshift has implemented Routes to solve this and k8's ha simplemented ingress later
-----------------------------
how ingress solves about issuse:
kubernetes said users will create Ingress and resource and can use ingress controllers from load balancers.

suppose we need path based routing: inside yaml we will mention routebased load balancer
and we must also deploy ingress controller(load balancer + API gateway) using yamlfiles,helm charts.

if ingress controller is missing no matter how many ingress resources we create it is waste. these ingress controller will check ingress resources and do that load balancer things.
----------------------------
ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-example
spec:
  rules:
  - host: "foo.bar.com"
    http:
      paths:
      - pathType: Prefix
        path: "/bar"
        backend:
          service:
            name: python-raghu-servive-one
            port:
              number: 80

-------------------------------------------------------------------------------------
kubectl get svc

kubectl apply -f ingress.yml

 kubectl get ingress


to install ingres in minikube: minikube addons enable ingress

to verify ingress installed or not: kubectl get pods -A | grep nginx
kubectl logs ingress-nginx-controller-768f948f8f-8hf6b -n ingress-nginx

kubectl get ingress


ingress class names: are used to define multople controllers (nhinx and HA-proxy)

routes does not support TLS certificates, so many use ingress instead of routes in openshift.


------------------------------------------------------------------------------------------------------------
RBAC: Role Based Accessed Control
RBAC is directly related to security

use case1: we can give access to users
RBAC can be divide din to Two parts 1)Users 2)Service Accounts
what acess developer and qe have access in namespace and etcd all this can be defined in RBAC.

use case2: what access should pod have 
what acess should pod have ,should it have acess to (1)Configmaps and (2)Secrets 

To k8's there are 3 major things for managing RBac's:
1)Service Accounts/Users
2)Roles/Cluster Role
3)Role Binding / Cluster Role Binding

-----------------------------------------------------------------
how do you create users in K8's

k8's does not deal with usermanagement. it offloads the usermanagement. in k8s the api-server works as oAuth server. 
suppose we are in Eks are any Identity provider,we can use IAM users, using Iam and providing and creating iam provider or oAuth Provider we can provide login access lo k8's.
Keycloak is one of the major provider identity acess provider.
supoose we can integrate EKS with Keycloak and using keycloak we can connect with github. using github we can do user management.
service accounts we just create yaml files (serviceaccount.yaml).
by default when a pod is created a default service account is associated with it automatically. and using service account only all the application running will be talking to API server.
to define access k8s defines 2 method (1) Role/Cluster Role(when cluster level permissions) (2)Role-Binding/Cluster Binding
----------------------
first we create role and say they should have access to pod, config maps and secrets , if thet have to have access to cluster we create cluster roles.
to attach this roles we uses role-binding.

-----------------------------------------------
we create service account and we create a role and using role we bind service account and role binding. with role we cant bind service account and role binding.

service account (it has users), roles (it has permissions) and role-binding (using this we bind users with permissions).

---------------------------------------------
Custom Resource Defnition (CRD),Custom Resources and custom Controller:

if you want to introduce a new resource to k8s, for example like we want more security to k8's we can use kubehunter, kube bench etc. we have apps like ARGO-CD, flux, spinnacare. ot
to extend the api of k8's.

1) devops: deploying the custom resource defnition and custom controller is the responsibility  of devops enginner
2) user: deploting the custom resource can be the action of user or devops enginner as well.

one we started exploring k8s we rialize they are many things to explote like for example

1) Istio : which gives service mesh capabilitties 
2) Argo -CD : which give gitops functionality.
3)Key cloak: Iam, OAuth functinality.
4) for securith we have kube-bench.

to handle all these additional capabilities users can extend the API of k8s by using CRD, Custom resurces and Custom Controller.

-> people of Istion will define a new resouce API to k8s by using a Custom Resouce defnition(CRD) to k8s. this CRD is yaml file in which we define what a user  can create for example
if we are a user who create deployment.yml file we mentioned API,kind,pod , template,container beyond this how does k8s understand the deplyment.yaml we are doing is correct or not 
like tomorrow we may add volumes or we may add new feature called XYZ. so k8s will have template/resource defnition already in controller when ever we  create fileld xyz kks will say field xyz is not known. k8's will have a defnition
out of this we can define what is required or omit which is not required. similarly for even custom resource before any one supports k8s ask for the file for example in that file
IStio writes what services it supports, like istio has virtual service which it supports(this virtual service is a custom resource). this custom resource is validated by Custom resource defnition which istio people
has submitted to k8s.

A CRD is yaml file which is used to introduce a new ApI to K8s that will have all the fields user can define in k8s.

suppose organisation decided to use istio:
step 1) first we deploy a custom resource defnition (crd) using plane k8s manifets/helm charts or by using operator. (now virtual service crd is deployed in k8s)
     2) suppose there is an user who creates custom resource in a namespace inside namespace he will create Istiovirtualservice custom resource from istio which is validated against CRD and gets deployed.
     3) it will just stay after validation not activated after deployment , next devops should deploy a custom controller acroos the cluster or only for that namespace. now custon controller,
          will monitor the custon resource user is created i.e Istiovirtualservice.

we can write a custom controller using golang, python or java. (k8s has API client-go which will interact with k8's API SERVER.). we will define or create new watchers and listers using client-go .
when ever update,delete and  create in k8s kubernetes has in built watchers to monitor. 


using controller-runtime we can set watchers in k8s, when ever there is crud this watchers will notify client-go
client go there is a package called reflector using reflector we can understand there is an update,create, delete we can put in worker fifo queue we start processing each queue resource.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
suppose out of k8, there is an backend app which needs to connect with db to get some info and give the output to enduser. to connect backend with db we need some info of db details for backend to connect like 
db port , db connection type etc and these need to be passed to backend but those should not be hardcoded as if details changed like any dbport the end user cant get details. 

for that generally we keep that credentials in a file and use it by importing file using os-import in python.
------------------

config Maps :

what is an config map in k8s:
To connect to DB in k8s suggets we create config-maps and keep details like db -port etc inside config-map. in containers (as part of environment variable or file like) k8's says we support config-maps. 
you can mount this confog-map,or the the details of config-map can be used inside pod as env variable or file,volume mounts but this information should come from config-map only.

so config-maps are used to store the information that can be used later by application,pod aor deployment



Secrets: secrets deal with sensitive data

suppose we store dbpassword in config-maps, when ever we create a resource then information is stored etcd. but in etcd the data is stored as objects. when ever there a hack then they can see the data in 
etcd. our entire platform is comprmised. to solve this we store sensitive data in secrets and non-sensitive data in config-maps.

if we store in secrets before object is stored in etcd  it will encrypt this at rest. by defaulit k8s uses default encryption we can also used custom encryption. because of it
even its hacked user cant read data as he has no encryption key.

As a user we create config-maps using yaml file we will use kubectl apply and create this config-map. at the same time api-server is saving this info in etcd.

if a hacker has access to k8s cluster he can come to config-maps and see the data by using kubectl describe or kubectl edit config maps. (1) so we have to keep in secrets which will be encrypted
(2) we should also use RBAC to control who can access to etcd.

(least previlage = only grant least acess very less people should  have access to etcd)
--------------------------------------------------------------------------------------------------
vim cm.yml
kubectl apply -f cm.yml

reply after above command: configmap/test-cm created

kubectl get cm
kubectl describe cm test-cm


in spec we have write of deployment file:
env:
  -name: DB-PORT
   valueFrom:
    ConfigMapKeyRef:
      name: test-cm
      key: db-port


changing the env variables inside the container is not possible it does not allow, we can only re create the container. once if we change dbport in config-map it will not change automatically.
in production we can restart the containers it may incur traffic loss. so other way k8 suggets is VolumeMounts(saved as files).

VolumeMount:
inside specs At the level of the containers you create volumes
--------------------
Volumes:
  -name: db-connection
   configMap:
     name: test-cm
-------------------------

and inside containers write below
------------------------
VolumeMounts: 
  -name : db-connection
   ConfigMap: 
   mountPath: /opt
------------------------------------


to create secrets:
$ kubectl create secret generic test-secret2 --from-literal=user-name="ragh"
secret/test-secret2 created

kubectl describe secret test-secret2

kubectl edit secret test-secret2
(it will encrypt in base64 we can also do custom if required)

we can use hashicorp vault or seal secrets for more customization of secure


know about etcd security encryption at etcd.


---------------------------------------------------------------------------------------------------------------------------------------------------

promotheus:

why monitoring:
as no of k8 clusters increases like dev, production we need promotheus to monitor, promotheus has a http server which collect all metrics that k8's API-SERVER exposes by default ans stores them
in a time-series database.



grafana: is for visulaization



















-------------------------------------------
note:

we use kubectl apply... over kuberctl create...
Because when apply cmd is used, we can simply kubectl edit... the concerned files, without applying again.
In kubectl create..., you can't use use kubectl edit, you need to manually change the YAML and and create again.
it stores on a node using hdd or ssd.
we can configure promtheus with alertmanager we can send alert to slack, if api server is not responsing then send to alertmanager, so the alert manager based on no of things we have setup we can send email,
sms.

we can go to promotheus Web UI and execute prompql queries or we can get from dashboard like grafana or any other.

grafana is just for data visualization with charts etc from data we get from promotheus.


bu default we get only metrics that k8 sends.if we nwwd we can customize.




























































